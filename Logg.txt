SemEval:

4. januar:
    PETTER:
    - Manuelt klassifisert noen få tweets i filen "stream__climatechange_clean.json" til bruk i label prop
     for å se om den klarer å klassifisere riktig. Veldig vanskelig å finne tweets som er against.
     Jeg tenkte å trene med 15 fra hver klasse, og teste med 5 fra hver klasse ("NONE" er tatt fra opprinnelig treningsset).
     == Resultatet viste seg å bli "NONE" eller "FAVOR" klassifisert... Altså vanskelig å predicte against.

     Tweetene jeg har manuelt sjekket har id og er tatt fra "stream__climatechange_clean.json:
     AGAINST:
     [662022121904840704, 662020009183551488, 662016401067155456, 662020009783373824, 662016718303195136]

     FAVOR:
     [662015471261245441, 662015310367752192, 662018031292256256, 662018963916722176, 662015805635325952]


    Koden for å hente ut tweetene i label_propagation.py:

    unlabeled_tweets = []
    unlabeledTweets = [662022121904840704, 662020009183551488, 662016401067155456, 662020009783373824, 662016718303195136,
                        662015471261245441, 662015310367752192, 662018031292256256, 662018963916722176, 662015805635325952]
    t = twitter.readTweetsAndIDs("stream__climatechange_clean")
    for tw in unlabeledTweets:
        for id in t:
            if tw == id[0]:
                unlabeled_tweets.append(id[1])

    HENRIK:
    - Laget ny og forbedret labelprop. Predicter nå alle klasser, men gjør alle av de manuelt labelede til in favor og
    ikke alle som blir imot gir mening. Foreslår at vi fortsatt benytter oss av den og ser hvordan det påvirker resultatet.
    (kan teste med forskjellige tweets osv.)
    Litt av problemet tror jeg ligger i språket kan variere så utrolig mye at vi trenger noen features som kan gjelder alle.
    Vi kan derfor se litt på de som jeg har listet i features.txt.

    - Laget liste med features som kan/bør brukes i klassifisering (og kanskje i label propagation) se features.txt.


5. januer:
    Plan:
    PETTER:
     -Se hva sentiment lexicon kan gjøre:
      Så langt prøvd å kjøre noen tweets (spesielt 'against' tweets) på algoritmen SentiStrength (http://sentistrength.wlv.ac.uk).
        Ikke veldig begeistret for resultatene. Virker som det er vanskelig å komme med en slutning for sammenheng
        mellom positivitet/negativitet og ståsted ut i fra SemEval treningsdata.
      Har også lastet ned Stanfords CoreNLP for å teste algoritmen Deeply Moving (sentiment analysis for filmanmeldelser).
        Testene gjort mot 'against' her ble ofte klassifisert som negativt (hvilket kunne vært positivt), men dessverre ble
        altfor mange 'favor' også negative. Dette er ofte på grunn av kontekst i teksten. i.e:
        - [against] folk mener ofte at climate change people er dumme etc.. [negativ sentiment]
        - [favor]   mens andre mener at regjeringen er håpløse etc.. [negativ sentiment]
      Resultatene fra testene tilsier at jeg ikke ser nytten av å implementere/importere et sentiment leksikon.

    HENRIK:
     -Se på features og gå over koden.
     Jeg har implementert tre features: negation (om det finnes nektelse eller ei i tweeten), lengthOfTweet (normalisert
     etter makslengde på tweet) og numberOfTokens (antall ord). I tillegg har jeg observert at å bruke 3-gram i tillegg
     til unigram ga bedre resultater. Vi er nå oppe på Macro F = 0.4000 for SVM på kun klima.

     Jeg har også sett på probabilities, noe som vi tenkte kunne gjøres med scikit, og det burde være mulig. Jeg har
     fått koden til å regne de ut (kommentert ut i linje 215-217) i bag_of_words2.0, nå gjenstår bare å bruke de.

     Sett over koden, fikset små bugs, og ryddet litt.

6. januar:
    PETTER:
    - Implementert number of capital words. Usikker om jeg skulle returnere float mellom 0 og 1 eller antall
    ord med store bokstaver. Endte med antall ord (lett å gjøre om hvis det trengs, men har kanskje ikke noe å si?)
    Ser at å droppe lemmatizing (sette den = 0) gir bittelitt bedre res (0.4046) :-)
    - Implementert punctuation marks som finner alle tegn etter hverandre større enn 1 som inneholder
    interpunksjonstegnene: !"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~
    Måten den løser om siste tegn er ? eller ! returner kun 0 eller 1 uavhengig av antall ord som slutter med
    ! eller ?, dvs feks:"Hei!!" og "Hei%%" vil bli returnert som henholdsvis [1, 1] og [1, 0]
    Det ser ut til at denne trekker litt ned. (0.3969)
    - Implementerer antall ord som er lengre enn vanlig.
    - Implementert sentiment av tweets. Flere maater aa loese dette pa. Hittil separerer jeg features neg,neu,pos men
    man kan f eks bruke en overall som gir en verdi mellom [-1, 1]. For aa kjoere denne maa du kanskje laste ned
    et text dokument. Se oeverst i processTrainingData.py for instruks :) Det ser ikke ut til at denne påvirker
    veldig mye.
    Jeg har ikke testet ut kombinasjoner eller lagt mer tyngde på spesielle features (noe som kan være viktig).
    Jeg ser ogsaa at sentiment.vader har en Booster_dict som man kan dra nytte av (http://www.nltk.org/_modules/nltk/sentiment/vader.html#SentimentIntensityAnalyzer.sentiment_valence).


    HENRIK:
    -Se på crossvalidation slik at vi får testet på en ordentlig måte.
    -Implementere flere features.
    -Inkludere og teste labelprop med koden vår.

12. januar:
    HENRIK:
    -Label propagation:
    Label propagation er nå implementert og tatt i bruk i trening. Kan slås av og på i settings. Ved trening på
    hele treningsettet og bruk av cross validation økes resultatet fra en mean på ish 0.54 til ish 0.60 (alle parametere
    beholdt som før dvs alt i bruk untatt abstracts, tdidf og bigram).
    Når label prop taes i bruk får vi også en økning i Macro F-score på 0.0033 (all treningsdata og Erwins test data).
    Når ikke test_train_split er i bruk taes hele treningsettet i bruk og vi bruker erwins "gold" til testing. Vi bør her
    kanskje fokusere på corss validation mean, siden den kanskje er bedre enn Erwins manuelle klassifisering.
    Det er verdt å nevne at hvis labelingen til Erwin stemmer gjør vi det dårligere en dummy...

    -Storing of fitted models:
    Lagt til metoder for å lagre og laste inn modeller som er trent.

    PETTER:
        - Lagt til test data.
        - Kjoerte foerste gang med test data (kun Climate topic)
        - Implementerte inn Erwin sin "annotated test data"

13 januar:
    PETTER:
     - Lagt til to nye features i form av antall pronomener i en tweet (tar ca 20 min aa kjoere med den...) og antall
     positive og negative ord i en tweet. Jeg gjorde om use_sentimentAnalyzer til aa lagre kun sentimentet for hele
     tweeten (compound).
     - lagt til Naive Bayes.
     - La til en metode som henter alle skeptiske tweeten fra skeptical science. Mulig den kan hjelpe.
     - Ser også ut som top hashtags kan hjelpe (i hvert fall for favor klassen) hvis vi booster disse litt ekstra.
     kanskje kopiere inn dem i alle favor tweetsene eller no :)

     HENRIK:
     - Lagt in koden til Erwin.
     - Lagt in utskrift av classification report.
     - Endret tilbake sentiment analyser da det ikke støttes negative features i naive bayes.
