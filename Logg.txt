SemEval:

4. januar:
    PETTER:
    - Manuelt klassifisert noen få tweets i filen "stream__climatechange_clean.json" til bruk i label prop
     for å se om den klarer å klassifisere riktig. Veldig vanskelig å finne tweets som er against.
     Jeg tenkte å trene med 15 fra hver klasse, og teste med 5 fra hver klasse ("NONE" er tatt fra opprinnelig treningsset).
     == Resultatet viste seg å bli "NONE" eller "FAVOR" klassifisert... Altså vanskelig å predicte against.

     Tweetene jeg har manuelt sjekket har id og er tatt fra "stream__climatechange_clean.json:
     AGAINST:
     [662022121904840704, 662020009183551488, 662016401067155456, 662020009783373824, 662016718303195136]

     FAVOR:
     [662015471261245441, 662015310367752192, 662018031292256256, 662018963916722176, 662015805635325952]


    Koden for å hente ut tweetene i label_propagation.py:

    unlabeled_tweets = []
    unlabeledTweets = [662022121904840704, 662020009183551488, 662016401067155456, 662020009783373824, 662016718303195136,
                        662015471261245441, 662015310367752192, 662018031292256256, 662018963916722176, 662015805635325952]
    t = twitter.readTweetsAndIDs("stream__climatechange_clean")
    for tw in unlabeledTweets:
        for id in t:
            if tw == id[0]:
                unlabeled_tweets.append(id[1])

    HENRIK:
    - Laget ny og forbedret labelprop. Predicter nå alle klasser, men gjør alle av de manuelt labelede til in favor og
    ikke alle som blir imot gir mening. Foreslår at vi fortsatt benytter oss av den og ser hvordan det påvirker resultatet.
    (kan teste med forskjellige tweets osv.)
    Litt av problemet tror jeg ligger i språket kan variere så utrolig mye at vi trenger noen features som kan gjelder alle.
    Vi kan derfor se litt på de som jeg har listet i features.txt.

    - Laget liste med features som kan/bør brukes i klassifisering (og kanskje i label propagation) se features.txt.


5. januer:
    Plan:
    PETTER:
     -Se hva sentiment lexicon kan gjøre:
      Så langt prøvd å kjøre noen tweets (spesielt 'against' tweets) på algoritmen SentiStrength (http://sentistrength.wlv.ac.uk).
        Ikke veldig begeistret for resultatene. Virker som det er vanskelig å komme med en slutning for sammenheng
        mellom positivitet/negativitet og ståsted ut i fra SemEval treningsdata.
      Har også lastet ned Stanfords CoreNLP for å teste algoritmen Deeply Moving (sentiment analysis for filmanmeldelser).
        Testene gjort mot 'against' her ble ofte klassifisert som negativt (hvilket kunne vært positivt), men dessverre ble
        altfor mange 'favor' også negative. Dette er ofte på grunn av kontekst i teksten. i.e:
        - [against] folk mener ofte at climate change people er dumme etc.. [negativ sentiment]
        - [favor]   mens andre mener at regjeringen er håpløse etc.. [negativ sentiment]
      Resultatene fra testene tilsier at jeg ikke ser nytten av å implementere/importere et sentiment leksikon.

    HENRIK:
     -Se på features og gå over koden.
     Jeg har implementert tre features: negation (om det finnes nektelse eller ei i tweeten), lengthOfTweet (normalisert
     etter makslengde på tweet) og numberOfTokens (antall ord). I tillegg har jeg observert at å bruke 3-gram i tillegg
     til unigram ga bedre resultater. Vi er nå oppe på Macro F = 0.4000 for SVM på kun klima.

     Jeg har også sett på probabilities, noe som vi tenkte kunne gjøres med scikit, og det burde være mulig. Jeg har
     fått koden til å regne de ut (kommentert ut i linje 215-217) i bag_of_words2.0, nå gjenstår bare å bruke de.

     Sett over koden, fikset små bugs, og ryddet litt.

6. januar:
    HENRIK:
    -Se på crossvalidation slik at vi får testet på en ordentlig måte.
    -Implementere flere features.
    -Inkludere og teste labelprop med koden vår.


